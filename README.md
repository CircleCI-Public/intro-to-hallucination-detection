# intro-to-hallucination-detection
Sample repository to accompany the deeplearning.ai course on evaluations.


## Setting up the project
Describe the sample project we’ll be using in the tutorial

## Walk through the setup steps

### Demonstrate how it works

### Example hallucination

#### Give a basic example of a hallucinated response 

Describe other ways hallucinations could affect different types of apps and the impact on users/developers/businesses

## Adding CI
Explain how an automated eval pipeline can help catch these types of issues before they reach users, accelerating your model tuning processes and increasing confidence in your delivery cycles

### CircleCI config file
Setting up a hallucination-detecting evaluation pipeline
Show off the CircleCI config file

##### Config walk through
Explain the key points in a bulleted list calling out the important lines

### Setup a project
Explain how to set the project up on CircleCI

### Running the pipeline
Update the model prompt to force a hallucination

#### A failing test
Demo the pipeline running and catching the error

#### Fixing the test
Fix the prompt (simulating what a prompt engineer might do in response)

Show the pipeline building green

## Conclusion
Recap what you learned and why it’s valuable

Encourage readers to learn more about automating LLM evaluations by visiting our DeepLearning.ai course

